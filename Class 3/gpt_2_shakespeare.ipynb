{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "1d1fecd5-c386-4b12-d36c-338e3cb86887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "0b092916-c801-4327-d83f-323b26cad22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 683Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.11Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 436Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 45.7Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 788Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.39Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.85Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "1a2873c9-2e06-484b-c7c0-1f4f6f809749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "41978740-7547-4658-c0df-6a03468eea0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "I don't know. I don't think I can understand that. I mean, I'm not saying it's a planet, but it's a planet with a planet. At the end of the day, we don't know what happened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "5bf0f57b-b7f8-40c7-ff13-76016d1a680d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:33:23--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.135.216, 52.216.53.152, 54.231.131.248, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.135.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-24 11:33:24 (4.45 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "ef17e1d3-ebaa-4018-f19c-0d78aa136105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 216MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "389c5340-3c17-4755-f7fd-64094a85fcf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:33:31--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-24 11:33:31 (22.1 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'nietzsche.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "8798df68-2d72-43fb-bcb6-d78fcc3a1a86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 143770 tokens\n",
            "Training...\n",
            "[1 | 6.54] loss=4.11 avg=4.11\n",
            "[2 | 8.68] loss=4.09 avg=4.10\n",
            "[3 | 10.81] loss=4.03 avg=4.07\n",
            "[4 | 12.95] loss=3.88 avg=4.03\n",
            "[5 | 15.09] loss=3.63 avg=3.94\n",
            "[6 | 17.24] loss=3.85 avg=3.93\n",
            "[7 | 19.39] loss=3.87 avg=3.92\n",
            "[8 | 21.55] loss=3.82 avg=3.91\n",
            "[9 | 23.71] loss=3.65 avg=3.88\n",
            "[10 | 25.89] loss=3.42 avg=3.83\n",
            "[11 | 28.06] loss=3.87 avg=3.83\n",
            "[12 | 30.24] loss=3.85 avg=3.83\n",
            "[13 | 32.43] loss=3.56 avg=3.81\n",
            "[14 | 34.62] loss=3.77 avg=3.81\n",
            "[15 | 36.81] loss=3.75 avg=3.80\n",
            "[16 | 39.00] loss=3.72 avg=3.80\n",
            "[17 | 41.21] loss=3.51 avg=3.78\n",
            "[18 | 43.42] loss=3.75 avg=3.78\n",
            "[19 | 45.64] loss=3.69 avg=3.77\n",
            "[20 | 47.86] loss=3.63 avg=3.77\n",
            "[21 | 50.09] loss=3.61 avg=3.76\n",
            "[22 | 52.32] loss=3.62 avg=3.75\n",
            "[23 | 54.56] loss=3.48 avg=3.74\n",
            "[24 | 56.81] loss=3.38 avg=3.72\n",
            "[25 | 59.05] loss=3.64 avg=3.72\n",
            "[26 | 61.31] loss=3.63 avg=3.71\n",
            "[27 | 63.56] loss=3.60 avg=3.71\n",
            "[28 | 65.82] loss=3.63 avg=3.70\n",
            "[29 | 68.09] loss=3.57 avg=3.70\n",
            "[30 | 70.37] loss=3.39 avg=3.69\n",
            "[31 | 72.65] loss=3.51 avg=3.68\n",
            "[32 | 74.95] loss=3.63 avg=3.68\n",
            "[33 | 77.23] loss=3.25 avg=3.66\n",
            "[34 | 79.53] loss=3.61 avg=3.66\n",
            "[35 | 81.84] loss=3.59 avg=3.66\n",
            "[36 | 84.14] loss=3.59 avg=3.66\n",
            "[37 | 86.46] loss=3.64 avg=3.66\n",
            "[38 | 88.78] loss=3.53 avg=3.65\n",
            "[39 | 91.11] loss=3.50 avg=3.65\n",
            "[40 | 93.44] loss=3.51 avg=3.64\n",
            "[41 | 95.77] loss=3.35 avg=3.64\n",
            "[42 | 98.10] loss=3.46 avg=3.63\n",
            "[43 | 100.43] loss=3.60 avg=3.63\n",
            "[44 | 102.76] loss=3.39 avg=3.62\n",
            "[45 | 105.09] loss=3.56 avg=3.62\n",
            "[46 | 107.41] loss=3.33 avg=3.61\n",
            "[47 | 109.72] loss=3.52 avg=3.61\n",
            "[48 | 112.04] loss=3.23 avg=3.60\n",
            "[49 | 114.34] loss=3.53 avg=3.60\n",
            "[50 | 116.64] loss=3.29 avg=3.59\n",
            "[51 | 118.95] loss=3.34 avg=3.58\n",
            "[52 | 121.25] loss=3.32 avg=3.58\n",
            "[53 | 123.54] loss=3.35 avg=3.57\n",
            "[54 | 125.84] loss=3.37 avg=3.57\n",
            "[55 | 128.13] loss=3.47 avg=3.57\n",
            "[56 | 130.42] loss=3.37 avg=3.56\n",
            "[57 | 132.71] loss=3.35 avg=3.56\n",
            "[58 | 134.99] loss=3.41 avg=3.55\n",
            "[59 | 137.28] loss=3.13 avg=3.54\n",
            "[60 | 139.57] loss=3.47 avg=3.54\n",
            "[61 | 141.86] loss=3.25 avg=3.54\n",
            "[62 | 144.16] loss=3.29 avg=3.53\n",
            "[63 | 146.45] loss=3.17 avg=3.52\n",
            "[64 | 148.74] loss=3.19 avg=3.52\n",
            "[65 | 151.03] loss=3.42 avg=3.51\n",
            "[66 | 153.32] loss=3.20 avg=3.51\n",
            "[67 | 155.62] loss=3.45 avg=3.51\n",
            "[68 | 157.91] loss=3.17 avg=3.50\n",
            "[69 | 160.20] loss=3.24 avg=3.49\n",
            "[70 | 162.49] loss=3.23 avg=3.49\n",
            "[71 | 164.79] loss=3.17 avg=3.48\n",
            "[72 | 167.09] loss=3.46 avg=3.48\n",
            "[73 | 169.39] loss=3.13 avg=3.48\n",
            "[74 | 171.69] loss=3.30 avg=3.47\n",
            "[75 | 173.99] loss=3.15 avg=3.47\n",
            "[76 | 176.29] loss=2.98 avg=3.46\n",
            "[77 | 178.59] loss=3.29 avg=3.45\n",
            "[78 | 180.90] loss=3.38 avg=3.45\n",
            "[79 | 183.20] loss=3.20 avg=3.45\n",
            "[80 | 185.51] loss=3.07 avg=3.44\n",
            "[81 | 187.81] loss=3.27 avg=3.44\n",
            "[82 | 190.12] loss=3.03 avg=3.43\n",
            "[83 | 192.43] loss=3.07 avg=3.42\n",
            "[84 | 194.73] loss=3.19 avg=3.42\n",
            "[85 | 197.03] loss=3.16 avg=3.42\n",
            "[86 | 199.33] loss=3.06 avg=3.41\n",
            "[87 | 201.63] loss=2.97 avg=3.40\n",
            "[88 | 203.94] loss=3.21 avg=3.40\n",
            "[89 | 206.23] loss=3.27 avg=3.40\n",
            "[90 | 208.53] loss=3.32 avg=3.40\n",
            "[91 | 210.83] loss=2.96 avg=3.39\n",
            "[92 | 213.12] loss=2.90 avg=3.38\n",
            "[93 | 215.42] loss=3.11 avg=3.38\n",
            "[94 | 217.72] loss=3.10 avg=3.37\n",
            "[95 | 220.02] loss=2.84 avg=3.36\n",
            "[96 | 222.32] loss=2.85 avg=3.35\n",
            "[97 | 224.61] loss=3.08 avg=3.35\n",
            "[98 | 226.91] loss=3.01 avg=3.34\n",
            "[99 | 229.22] loss=3.04 avg=3.34\n",
            "[100 | 231.51] loss=2.97 avg=3.33\n",
            "======== SAMPLE 1 ========\n",
            " disparive to all sorts of moral and religious sentiments, even to the most unwholesome sentiments! I am now a \"Gross-Homer\"! It has not become the \"God, Good, and Evil,\" by which I mean \"good\" and \"bad,\" \"sinners\" and \"lover's errands\"--it has just become my own \"God, Good, and Evil,\"--but there IS no need of your \"good\" and \"bad\" interpretations as \"Gravitational\"\n",
            "appears.--\"Here we are not as we can be.--To be moral and good means to \"be a good man, a good-for-all and to \"love God\"\n",
            "or to love \"all those things which are good.\" To be moral and good means to love \"one's neighbor as oneself,\n",
            "ourselves as others (Gross Hombres)\" or to love some \"noble, rational entity,\"\n",
            "or \"a being not all things,\" to be a \"universal species,\" and be\n",
            "contemplative and to look out for others, all from the standpoint of the\n",
            "same moral and/or religious ideal, although we are all, in the opinion of\n",
            "an extraordinary number of people nowadays, profoundly religious, \"mystery ones,\"\n",
            "or \"the little things of the world\"--and in certain cases also \"nobles of the earth, the\n",
            "earth\"--I would say that, in fact, here, in certain people and\n",
            "in others, in all \"moral\" ideas and feelings, an infinite multitude of very\n",
            "subtle, yet very long-suffering, painless spirits, that roam among\n",
            "things, and that are often as men, but rarely as people,--and \"nobles of earth\"\n",
            "(not to speak of \"us,\" not to be mistaken with \"sons and mothers\"\n",
            "for whatever they are, who, in a certain sense, serve as \"us\"--\"ourselves,\"\n",
            "we all \"ourselves\"),--has now become such a thing, and that the whole\n",
            "world-spirit may now grow, for all time as it were, too proud of its\n",
            "\"good\" and \"evil\" interpretations, too proud of being \"soul\" in\n",
            "itself, too proud to love \"itsself\" as itself, in any respect whatsoever.--All these\n",
            "individuals, perhaps even all humanity, now in the domain of morality, have,\n",
            "without any further hesitation, given each other their opinions--and the\n",
            "soul has a conscience now. That which I am so fond of--an immense sympathy, and\n",
            "an unswervingly inquisitive skepticism, for the sake of \"nobles\"--is to be understood under\n",
            "the strongest circumstances--that is, as though one really WAS \"NONE.\" (I\n",
            "never thought that this sympathy might have been stronger--I only understood it.--It is\n",
            "just as if one had \"nones,\" too--in a language much like our own.) To have \"nobles\"\n",
            "belongs very much to the origin of morality.\n",
            "\n",
            "\n",
            "13.\n",
            "\n",
            "=From the Philosophical Point of View It is only by virtue of some great\n",
            "decadence of \"nature\" and \"spirit\" that humanity has reached a place\n",
            "long disputed, and thus perhaps one can now say with some certainty: one\n",
            "cannot, in the long term, and also in spite of some certainty, still continue\n",
            "to be one hundred per cent noble when speaking of \"nature,\" and\n",
            "\"nature\" in general.\n",
            "\n",
            "\n",
            "14.\n",
            "\n",
            "=Metaphysic, or Metaphysic of Moral Morality.--Perhaps a syllable counts as the\n",
            "sentence of the sentence? \"But when we say that we are no longer no longer\n",
            "no longer 'nature,' how much more do we mean that everything\n",
            "is just as we once were? And if we once were 'nothing,' if there were\n",
            "no laws as to what constitutes the 'thing' which has not been\n",
            "sucked from us?\" How far we have to go with this categorical formula:\n",
            "\"The thing is merely our \"nature\" or \"seness,\" and can be\n",
            "\"any\"--but we are ourselves all this 'nature' and \"seness' with \"our\"\n",
            "self.\" What do we call ourselves \"Nature,\" after all, does\n",
            "no doubt we have, indeed, even at this present stage of time, this \"nature with \"our\"\n",
            "self,\" this thing that makes itself known and unrecognisably called \"Nature with\n",
            "\"We\"--the \"Nature with 'We'\"? And why does the idea of having\n",
            "\"Nature with 'We'\" and all its qualities, such a feeling and likeness in us,\n",
            "always, so long a faculty of reflection on the part of the conscious philosopher and\n",
            "a sort of philosophical pedagogue,--that is the idea already given us by the\n",
            "\"Nature with\n",
            "\n",
            "[101 | 245.61] loss=3.04 avg=3.33\n",
            "[102 | 247.90] loss=3.05 avg=3.32\n",
            "[103 | 250.19] loss=3.35 avg=3.33\n",
            "[104 | 252.49] loss=3.29 avg=3.32\n",
            "[105 | 254.78] loss=3.10 avg=3.32\n",
            "[106 | 257.07] loss=3.18 avg=3.32\n",
            "[107 | 259.37] loss=2.85 avg=3.31\n",
            "[108 | 261.66] loss=3.14 avg=3.31\n",
            "[109 | 263.96] loss=3.25 avg=3.31\n",
            "[110 | 266.26] loss=2.97 avg=3.30\n",
            "[111 | 268.55] loss=2.96 avg=3.30\n",
            "[112 | 270.85] loss=2.96 avg=3.29\n",
            "[113 | 273.14] loss=2.89 avg=3.29\n",
            "[114 | 275.44] loss=2.62 avg=3.28\n",
            "[115 | 277.75] loss=3.01 avg=3.27\n",
            "[116 | 280.04] loss=2.62 avg=3.26\n",
            "[117 | 282.34] loss=2.97 avg=3.26\n",
            "[118 | 284.64] loss=2.77 avg=3.25\n",
            "[119 | 286.93] loss=2.67 avg=3.24\n",
            "[120 | 289.22] loss=2.97 avg=3.24\n",
            "[121 | 291.53] loss=2.88 avg=3.24\n",
            "[122 | 293.83] loss=2.68 avg=3.23\n",
            "[123 | 296.12] loss=2.73 avg=3.22\n",
            "[124 | 298.42] loss=2.91 avg=3.22\n",
            "[125 | 300.72] loss=2.55 avg=3.21\n",
            "[126 | 303.02] loss=2.63 avg=3.20\n",
            "[127 | 305.32] loss=2.99 avg=3.20\n",
            "[128 | 307.61] loss=2.69 avg=3.19\n",
            "[129 | 309.91] loss=2.89 avg=3.19\n",
            "[130 | 312.21] loss=3.09 avg=3.18\n",
            "[131 | 314.52] loss=2.51 avg=3.17\n",
            "[132 | 316.81] loss=2.76 avg=3.17\n",
            "[133 | 319.11] loss=2.70 avg=3.16\n",
            "[134 | 321.40] loss=3.06 avg=3.16\n",
            "[135 | 323.70] loss=2.87 avg=3.16\n",
            "[136 | 326.01] loss=2.34 avg=3.15\n",
            "[137 | 328.31] loss=2.84 avg=3.14\n",
            "[138 | 330.60] loss=2.75 avg=3.14\n",
            "[139 | 332.91] loss=2.72 avg=3.13\n",
            "[140 | 335.20] loss=2.80 avg=3.13\n",
            "[141 | 337.50] loss=2.78 avg=3.12\n",
            "[142 | 339.80] loss=2.68 avg=3.12\n",
            "[143 | 342.10] loss=2.58 avg=3.11\n",
            "[144 | 344.39] loss=2.74 avg=3.10\n",
            "[145 | 346.69] loss=2.60 avg=3.10\n",
            "[146 | 348.99] loss=2.60 avg=3.09\n",
            "[147 | 351.30] loss=2.68 avg=3.09\n",
            "[148 | 353.60] loss=2.83 avg=3.08\n",
            "[149 | 355.90] loss=2.43 avg=3.07\n",
            "[150 | 358.20] loss=2.66 avg=3.07\n",
            "[151 | 360.50] loss=2.65 avg=3.06\n",
            "[152 | 362.80] loss=2.86 avg=3.06\n",
            "[153 | 365.10] loss=2.20 avg=3.05\n",
            "[154 | 367.40] loss=2.29 avg=3.04\n",
            "[155 | 369.69] loss=2.75 avg=3.04\n",
            "[156 | 371.99] loss=2.43 avg=3.03\n",
            "[157 | 374.29] loss=2.48 avg=3.02\n",
            "[158 | 376.59] loss=2.71 avg=3.02\n",
            "[159 | 378.89] loss=2.58 avg=3.01\n",
            "[160 | 381.18] loss=2.30 avg=3.00\n",
            "[161 | 383.48] loss=2.61 avg=3.00\n",
            "[162 | 385.78] loss=2.71 avg=3.00\n",
            "[163 | 388.08] loss=2.45 avg=2.99\n",
            "[164 | 390.38] loss=2.39 avg=2.98\n",
            "[165 | 392.69] loss=2.69 avg=2.98\n",
            "[166 | 394.98] loss=2.54 avg=2.97\n",
            "[167 | 397.28] loss=2.56 avg=2.97\n",
            "[168 | 399.59] loss=2.54 avg=2.96\n",
            "[169 | 401.89] loss=2.46 avg=2.96\n",
            "[170 | 404.20] loss=2.36 avg=2.95\n",
            "[171 | 406.50] loss=2.27 avg=2.94\n",
            "[172 | 408.80] loss=2.28 avg=2.93\n",
            "[173 | 411.10] loss=2.75 avg=2.93\n",
            "[174 | 413.41] loss=2.48 avg=2.93\n",
            "[175 | 415.72] loss=2.40 avg=2.92\n",
            "[176 | 418.01] loss=2.46 avg=2.91\n",
            "[177 | 420.32] loss=2.47 avg=2.91\n",
            "[178 | 422.61] loss=2.30 avg=2.90\n",
            "[179 | 424.92] loss=1.99 avg=2.89\n",
            "[180 | 427.22] loss=2.28 avg=2.88\n",
            "[181 | 429.52] loss=2.20 avg=2.87\n",
            "[182 | 431.83] loss=2.19 avg=2.87\n",
            "[183 | 434.13] loss=2.56 avg=2.86\n",
            "[184 | 436.44] loss=1.95 avg=2.85\n",
            "[185 | 438.74] loss=2.21 avg=2.84\n",
            "[186 | 441.04] loss=2.07 avg=2.83\n",
            "[187 | 443.33] loss=2.15 avg=2.83\n",
            "[188 | 445.63] loss=2.17 avg=2.82\n",
            "[189 | 447.94] loss=2.13 avg=2.81\n",
            "[190 | 450.24] loss=2.13 avg=2.80\n",
            "[191 | 452.55] loss=2.07 avg=2.79\n",
            "[192 | 454.85] loss=2.24 avg=2.79\n",
            "[193 | 457.15] loss=2.02 avg=2.78\n",
            "[194 | 459.44] loss=2.17 avg=2.77\n",
            "[195 | 461.75] loss=1.92 avg=2.76\n",
            "[196 | 464.06] loss=2.24 avg=2.76\n",
            "[197 | 466.36] loss=2.04 avg=2.75\n",
            "[198 | 468.66] loss=2.21 avg=2.74\n",
            "[199 | 470.96] loss=1.91 avg=2.73\n",
            "[200 | 473.26] loss=1.91 avg=2.72\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "Indeed,\n",
            "the idea of eternal time has hitherto been almost as repulsive to so lofty a\n",
            "nostreligion as to be foreign to Schopenhauer's taste. That eternal\n",
            "time, however, cannot altogether disentangle itself from nature and thereby\n",
            "sees the world in its finest aspects, and gives rise to the idea of a\n",
            "world-trotting higher than ourselves, is a great gain to Schopenhauer and his\n",
            "conception of the world. But what if he also wished to become a philosopher?\n",
            "And if all that the philosophers above us all have in common, WE all\n",
            "have in common, and all want in common, the same endocracy of Schopenhauer?\n",
            "Why should we hesitate in the face of this endocracy? The philosopher must\n",
            "make an indelible impression on our conscience: that is certainly something\n",
            "truly German, and something which German Gods and Max Weber have in common!\n",
            "But why should we, after having hitherto made an exception only in\n",
            "the music of Schopenhauer and Schelling, undertake to establish a doctrine which\n",
            "must, in German taste, as in the traditional music of Schopenhauer and Schelling,\n",
            "be the most attractive German doctrine to the whole of the German spirit?\n",
            "By means of a synthesis of German theology with the traditional music of\n",
            "the philosophers, that which is to be desired in all German music, is an\n",
            "achievement for the unification of the soul, an encouragement to the capacity for\n",
            "worship and self worship, as well as for self sacrifice and self reconciliation.\n",
            "Is not the German spirit, in its joyous, languid fugue-tail,--that is its charm and\n",
            "hindrance--the same in beauty?--is it not the same as the wind-forest of\n",
            "Pascal August my grandfather's background, which had its basis in what is now\n",
            "Russia? It is not the same as the sunshine-forest of the Tartars, as a\n",
            "fugue-tail for the wind;--it is certainly not the same as the TURKS OF\n",
            "LONG SED NAUS. Indeed, it is the same with the tributes to the GREAT. To the\n",
            "great, noble, good, and false Socrates, to the God, the TURKS OF\n",
            "Socrates, and the FILTH OF EVERY SYMPATHY--as the great, noble souls of\n",
            "\"high culture\"--is not the same as a SUOMELESS SAGE INHERENTITY, as some\n",
            "\"Tacitus Schillacean.\"--As regards the \"great, noble\" virtues of\n",
            "\"great\" culture, and the pedants and rakes, and the bad-moumoumers, and,\n",
            "one hopes, every dull-natured man of \"class\"--is NOT the same as\n",
            "a \"class,\" and is IT NOT possible that we Germans of some future\n",
            "century, who have, together with our \"greater good\"--the Germans of the\n",
            "future--should still retain SUOMELESS culture and virtue according\n",
            "to OUR TIME:--it Would NOT SUFFER TO DENY It, it WOULD CONSIDER It,--I\n",
            "say it with my whole heart--that people should persist in questioning this\n",
            "pessimism: that this great ideal, this \"greatness,\" which in OUR\n",
            "times HASEN'T HAPPENED, has ITS basis in the tragedy of a doomed\n",
            "personality, and thus has SUOMELESS virtue been sought after, by the\n",
            "great, proud, noble souls of today and hereafter: it WOULD BE SIMILAR TO\n",
            "PRESENT ITSELF: we Germans of the future, what! ye have only SUOMELESS\n",
            "valuation, and a certain inability to put your finger on the \"SIMILAR VALUE?\"\n",
            "ye doubt, ye insolence-in-fingerbearers! If ye have been thus \"concluded\"\n",
            "upon, ye hypocrites, ye insolence-in-fingerbearers, ye fools, ye\n",
            "citizen-spooks who have remained in \"history's past,\" ye now possess an\n",
            "already prepossessed strength and fortitude for the protracted \"bell-ring\n",
            "that\"s itself by virtue of which history has longed and needs education,\n",
            "and whereby ye may at last venture to examine anew the \"bell-ring\"\n",
            "be it stored up or retired: in short, to make a RESULTS-CONDITION\n",
            "OF THE REVERB, the \"THIRD REVERB\": a resolute RESULTS-CONDITION--perhaps\n",
            "a RESULTS-CONDITION--for mankind--ye shall \"foresee,\" ye will \"forelearn,\" ye will\n",
            "work, ye shall conduct yourselves, ye shall seek--well! there are still\n",
            "work-arts, manuals, and dysnourishment-curiosities to\n",
            "\n",
            "[201 | 486.72] loss=2.36 avg=2.72\n",
            "[202 | 489.02] loss=2.15 avg=2.71\n",
            "[203 | 491.31] loss=2.13 avg=2.70\n",
            "[204 | 493.61] loss=1.77 avg=2.69\n",
            "[205 | 495.91] loss=2.21 avg=2.69\n",
            "[206 | 498.21] loss=2.53 avg=2.69\n",
            "[207 | 500.52] loss=1.84 avg=2.68\n",
            "[208 | 502.81] loss=1.86 avg=2.67\n",
            "[209 | 505.11] loss=2.08 avg=2.66\n",
            "[210 | 507.41] loss=1.85 avg=2.65\n",
            "[211 | 509.72] loss=2.27 avg=2.65\n",
            "[212 | 512.02] loss=1.89 avg=2.64\n",
            "[213 | 514.31] loss=2.46 avg=2.64\n",
            "[214 | 516.61] loss=2.03 avg=2.63\n",
            "[215 | 518.92] loss=1.66 avg=2.62\n",
            "[216 | 521.22] loss=2.23 avg=2.61\n",
            "[217 | 523.53] loss=1.93 avg=2.61\n",
            "[218 | 525.83] loss=2.06 avg=2.60\n",
            "[219 | 528.13] loss=2.38 avg=2.60\n",
            "[220 | 530.43] loss=1.85 avg=2.59\n",
            "[221 | 532.73] loss=1.85 avg=2.58\n",
            "[222 | 535.02] loss=1.68 avg=2.57\n",
            "[223 | 537.33] loss=1.93 avg=2.56\n",
            "[224 | 539.62] loss=1.70 avg=2.55\n",
            "[225 | 541.92] loss=1.67 avg=2.54\n",
            "[226 | 544.23] loss=2.16 avg=2.54\n",
            "[227 | 546.53] loss=2.11 avg=2.54\n",
            "[228 | 548.84] loss=1.68 avg=2.53\n",
            "[229 | 551.15] loss=1.69 avg=2.52\n",
            "[230 | 553.44] loss=1.47 avg=2.51\n",
            "[231 | 555.74] loss=2.10 avg=2.50\n",
            "[232 | 558.04] loss=2.01 avg=2.50\n",
            "[233 | 560.34] loss=1.51 avg=2.48\n",
            "[234 | 562.65] loss=1.75 avg=2.48\n",
            "[235 | 564.95] loss=1.57 avg=2.47\n",
            "[236 | 567.25] loss=1.59 avg=2.46\n",
            "[237 | 569.54] loss=2.74 avg=2.46\n",
            "[238 | 571.84] loss=1.81 avg=2.45\n",
            "[239 | 574.15] loss=1.70 avg=2.44\n",
            "[240 | 576.45] loss=1.99 avg=2.44\n",
            "[241 | 578.74] loss=1.62 avg=2.43\n",
            "[242 | 581.04] loss=1.90 avg=2.42\n",
            "[243 | 583.34] loss=1.57 avg=2.42\n",
            "[244 | 585.65] loss=1.43 avg=2.40\n",
            "[245 | 587.95] loss=1.95 avg=2.40\n",
            "[246 | 590.25] loss=1.34 avg=2.39\n",
            "[247 | 592.54] loss=1.27 avg=2.38\n",
            "[248 | 594.84] loss=1.35 avg=2.36\n",
            "[249 | 597.14] loss=1.59 avg=2.36\n",
            "[250 | 599.45] loss=1.23 avg=2.34\n",
            "[251 | 601.74] loss=1.50 avg=2.33\n",
            "[252 | 604.04] loss=1.56 avg=2.33\n",
            "[253 | 606.34] loss=1.74 avg=2.32\n",
            "[254 | 608.64] loss=1.39 avg=2.31\n",
            "[255 | 610.95] loss=1.53 avg=2.30\n",
            "[256 | 613.25] loss=1.39 avg=2.29\n",
            "[257 | 615.55] loss=1.16 avg=2.28\n",
            "[258 | 617.85] loss=1.59 avg=2.27\n",
            "[259 | 620.15] loss=1.55 avg=2.26\n",
            "[260 | 622.46] loss=1.37 avg=2.25\n",
            "[261 | 624.76] loss=1.50 avg=2.25\n",
            "[262 | 627.06] loss=1.28 avg=2.24\n",
            "[263 | 629.36] loss=1.41 avg=2.23\n",
            "[264 | 631.65] loss=1.70 avg=2.22\n",
            "[265 | 633.96] loss=1.25 avg=2.21\n",
            "[266 | 636.27] loss=1.59 avg=2.20\n",
            "[267 | 638.57] loss=1.56 avg=2.20\n",
            "[268 | 640.87] loss=1.40 avg=2.19\n",
            "[269 | 643.17] loss=1.69 avg=2.18\n",
            "[270 | 645.47] loss=1.29 avg=2.17\n",
            "[271 | 647.78] loss=1.19 avg=2.16\n",
            "[272 | 650.07] loss=1.34 avg=2.15\n",
            "[273 | 652.37] loss=1.51 avg=2.15\n",
            "[274 | 654.67] loss=1.51 avg=2.14\n",
            "[275 | 656.96] loss=1.11 avg=2.13\n",
            "[276 | 659.27] loss=1.16 avg=2.12\n",
            "[277 | 661.58] loss=1.29 avg=2.11\n",
            "[278 | 663.87] loss=1.36 avg=2.10\n",
            "[279 | 666.17] loss=1.58 avg=2.10\n",
            "[280 | 668.47] loss=0.88 avg=2.08\n",
            "[281 | 670.77] loss=1.19 avg=2.07\n",
            "[282 | 673.08] loss=1.60 avg=2.07\n",
            "[283 | 675.37] loss=1.42 avg=2.06\n",
            "[284 | 677.66] loss=1.09 avg=2.05\n",
            "[285 | 679.96] loss=1.13 avg=2.04\n",
            "[286 | 682.26] loss=1.53 avg=2.04\n",
            "[287 | 684.57] loss=1.09 avg=2.03\n",
            "[288 | 686.87] loss=1.18 avg=2.02\n",
            "[289 | 689.17] loss=1.22 avg=2.01\n",
            "[290 | 691.46] loss=0.85 avg=2.00\n",
            "[291 | 693.76] loss=1.24 avg=1.99\n",
            "[292 | 696.06] loss=0.81 avg=1.98\n",
            "[293 | 698.37] loss=1.16 avg=1.97\n",
            "[294 | 700.66] loss=1.11 avg=1.96\n",
            "[295 | 702.95] loss=0.88 avg=1.95\n",
            "[296 | 705.25] loss=1.07 avg=1.94\n",
            "[297 | 707.55] loss=1.63 avg=1.94\n",
            "[298 | 709.87] loss=1.17 avg=1.93\n",
            "[299 | 712.17] loss=1.17 avg=1.92\n",
            "[300 | 714.46] loss=1.20 avg=1.91\n",
            "======== SAMPLE 1 ========\n",
            "ANKURE AND APPEARANCE\n",
            "in the form of anTIPS.--But this does NOT mean that all the artists, thinkers, and\n",
            "exemplars of former times are still to be such; this means that still other\n",
            "things, which we still call REVOLutions, are only a few centuries\n",
            "off:--they consist ultimately of a depreciation of the good human, which\n",
            "always results, in effect, from the depreciation of the good artistic\n",
            "individual: consequently, an increasing tendency is to degenerate into\n",
            "rapid deterioration, with the development of the type A, which, with the exception\n",
            "of certain Ascetic philosophers, must necessarily be regarded, as,\n",
            "so far as they are concerned, mediocre men who only strive for the\n",
            "MAJOR PROFOUND, merely for position, nothing more. There are, however,\n",
            "a great number of exceptional artists who, according to their condition,\n",
            "therefore, are in all honesty regarded as failures, just as are\n",
            "the mediocre artists who strive for the MEDIOCRITY of all appearances,\n",
            "as though some claim an almost \"metaphysical\" claim at the basis of\n",
            "themselves. On the other hand, there are those, notwithstanding, who would\n",
            "definitely regard themselves as masters of the occult and would\n",
            "have regard even to the \"metaphysical\" to-day, inasmuch as, notwithstanding\n",
            "all their attempts at deception and misunderstanding--including also their\n",
            "irrestrances and assistants--it is always just a matter of degree\n",
            "and object: for there are degrees for every difficulty, and also some\n",
            "for aesthetic and social ills. On the other hand, though there be equally\n",
            "many artists who simultaneously make art and life themselves impossible\n",
            "for each to in his or her situation, nevertheless owing to a strong inclinations\n",
            "for and against making art and life itself impossible, nevertheless\n",
            "taking art as a gift from the natural--in view of the fact that it is by no\n",
            " means the only gift from the natural that has not hitherto flowed freely\n",
            "through all the centuries; while still others, owing to an instinct for\n",
            "transfiguration and for the symbolism of things, either\n",
            "prefer not to make art themselves known, or in a reverent and humble manner\n",
            "do no honour to the natural, but merely wish to show what it is not; while\n",
            "on the other hand, if there be still still still sufficient time\n",
            "left in the sharpened senses--FOR instance, in the sharpness of the instincts\n",
            "and the emotions--the sharpening of the sharp senses is the best means of\n",
            "preventing the natural from appearing as the imitation of the real; while\n",
            "in judging between the unnatural and the inoffensive, as a sort of morality\n",
            "ARTENSALLY PASSES ALONG THE EFFECTIVELY, although in many cases--an even\n",
            "amount to a degenerating of the effect--it is generally concluded that because\n",
            "a certain impression of the natural is made of the hypocrite, or the\n",
            "unclean, the magician, or the crazy man, the effect is injured,\n",
            "and in general, bad. There is a point of diseased tolerance and a\n",
            "dangerous incapacity in every artist, a point of diseased\n",
            "tendency in every love, a cardinal rule of art.\n",
            "\n",
            "\n",
            "102\n",
            "\n",
            "If one should turn away from the picture in which one views life as it\n",
            "is, and draw the portrait of Socrates in it, you will be troubled\n",
            "not just at the picture, but even with the aspect--that is, the aspect of\n",
            "Socrates, the living soul, which is enclosed in that picture. The\n",
            "exoteric picture is the sign of wisdom; theoteric wisdom is fear.\n",
            "All philosophy has always been an exoteric religion, a crude and\n",
            "immoral education, in order to an certain extent blind one's eyes and\n",
            "have a hard time grasping the true religion. That which we now call\n",
            "\"intellectual freedom\"--that seems to us to be an over-subscribed relic\n",
            "of the eighteenth century--is still closely related to our modern\n",
            "philosophy, which takes everything, wherever it may lead, into the\n",
            "universe of the spirit. Even Plato, the greatest intellectual of\n",
            "the time, though in the habit of departing from the strictest ethics as a\n",
            "longingly maintaining\n",
            "law of utility--and of taking what was demanded of him as a\n",
            "fundamental belief, a thousand times. It is possible to have in\n",
            "the foreground a picture of the most modern of all possible ages in\n",
            "which the spirit may be foregrounded, perhaps foregrounded a bit further\n",
            "to obscure things from the point of view of utility. But for the\n",
            "present we have to go even further back: there are, nevertheless,\n",
            "really quite a number of people, even a remarkable number of philosophers, who see\n",
            "the task of explaining the spirit in the historical sense as essentially\n",
            "the same as the sense-unconditioning\n",
            "\n",
            "[301 | 727.89] loss=1.13 avg=1.90\n",
            "[302 | 730.19] loss=1.21 avg=1.90\n",
            "[303 | 732.49] loss=1.16 avg=1.89\n",
            "[304 | 734.78] loss=1.09 avg=1.88\n",
            "[305 | 737.08] loss=1.23 avg=1.87\n",
            "[306 | 739.38] loss=1.36 avg=1.87\n",
            "[307 | 741.67] loss=0.99 avg=1.86\n",
            "[308 | 743.97] loss=1.43 avg=1.85\n",
            "[309 | 746.27] loss=1.47 avg=1.85\n",
            "[310 | 748.57] loss=1.28 avg=1.84\n",
            "[311 | 750.87] loss=0.84 avg=1.83\n",
            "[312 | 753.16] loss=0.91 avg=1.82\n",
            "[313 | 755.46] loss=1.03 avg=1.82\n",
            "[314 | 757.76] loss=0.88 avg=1.81\n",
            "[315 | 760.06] loss=0.80 avg=1.80\n",
            "[316 | 762.36] loss=0.81 avg=1.78\n",
            "[317 | 764.65] loss=0.98 avg=1.78\n",
            "[318 | 766.95] loss=1.12 avg=1.77\n",
            "[319 | 769.25] loss=0.79 avg=1.76\n",
            "[320 | 771.55] loss=0.89 avg=1.75\n",
            "[321 | 773.84] loss=0.53 avg=1.74\n",
            "[322 | 776.14] loss=0.79 avg=1.73\n",
            "[323 | 778.43] loss=0.60 avg=1.72\n",
            "[324 | 780.74] loss=0.95 avg=1.71\n",
            "[325 | 783.05] loss=0.79 avg=1.70\n",
            "[326 | 785.35] loss=1.40 avg=1.70\n",
            "[327 | 787.65] loss=0.90 avg=1.69\n",
            "[328 | 789.94] loss=1.12 avg=1.68\n",
            "[329 | 792.24] loss=0.96 avg=1.67\n",
            "[330 | 794.54] loss=0.97 avg=1.67\n",
            "[331 | 796.84] loss=0.82 avg=1.66\n",
            "[332 | 799.13] loss=0.69 avg=1.65\n",
            "[333 | 801.43] loss=0.82 avg=1.64\n",
            "[334 | 803.71] loss=0.88 avg=1.63\n",
            "[335 | 806.02] loss=0.69 avg=1.62\n",
            "[336 | 808.33] loss=0.65 avg=1.61\n",
            "[337 | 810.63] loss=0.71 avg=1.60\n",
            "[338 | 812.92] loss=0.90 avg=1.60\n",
            "[339 | 815.22] loss=1.03 avg=1.59\n",
            "[340 | 817.52] loss=0.59 avg=1.58\n",
            "[341 | 819.83] loss=0.90 avg=1.57\n",
            "[342 | 822.13] loss=1.12 avg=1.57\n",
            "[343 | 824.42] loss=0.58 avg=1.56\n",
            "[344 | 826.72] loss=0.86 avg=1.55\n",
            "[345 | 829.02] loss=0.73 avg=1.54\n",
            "[346 | 831.33] loss=0.72 avg=1.53\n",
            "[347 | 833.64] loss=0.76 avg=1.52\n",
            "[348 | 835.94] loss=0.72 avg=1.52\n",
            "[349 | 838.24] loss=0.62 avg=1.51\n",
            "[350 | 840.54] loss=0.77 avg=1.50\n",
            "[351 | 842.84] loss=0.65 avg=1.49\n",
            "[352 | 845.15] loss=0.72 avg=1.48\n",
            "[353 | 847.46] loss=0.82 avg=1.48\n",
            "[354 | 849.77] loss=0.54 avg=1.47\n",
            "[355 | 852.07] loss=0.59 avg=1.46\n",
            "[356 | 854.37] loss=0.73 avg=1.45\n",
            "[357 | 856.68] loss=0.54 avg=1.44\n",
            "[358 | 858.99] loss=0.80 avg=1.43\n",
            "[359 | 861.29] loss=0.60 avg=1.43\n",
            "[360 | 863.59] loss=0.69 avg=1.42\n",
            "[361 | 865.90] loss=0.54 avg=1.41\n",
            "[362 | 868.20] loss=0.61 avg=1.40\n",
            "[363 | 870.51] loss=0.68 avg=1.39\n",
            "[364 | 872.82] loss=0.44 avg=1.38\n",
            "[365 | 875.12] loss=0.40 avg=1.37\n",
            "[366 | 877.42] loss=0.47 avg=1.36\n",
            "[367 | 879.73] loss=0.60 avg=1.36\n",
            "[368 | 882.04] loss=0.67 avg=1.35\n",
            "[369 | 884.34] loss=0.59 avg=1.34\n",
            "[370 | 886.64] loss=0.54 avg=1.33\n",
            "[371 | 888.95] loss=0.58 avg=1.33\n",
            "[372 | 891.25] loss=0.50 avg=1.32\n",
            "[373 | 893.55] loss=0.71 avg=1.31\n",
            "[374 | 895.85] loss=0.79 avg=1.31\n",
            "[375 | 898.15] loss=0.41 avg=1.30\n",
            "[376 | 900.46] loss=0.40 avg=1.29\n",
            "[377 | 902.75] loss=0.40 avg=1.28\n",
            "[378 | 905.06] loss=0.44 avg=1.27\n",
            "[379 | 907.37] loss=0.50 avg=1.26\n",
            "[380 | 909.66] loss=0.32 avg=1.25\n",
            "[381 | 911.96] loss=0.44 avg=1.24\n",
            "[382 | 914.26] loss=0.40 avg=1.24\n",
            "[383 | 916.56] loss=0.39 avg=1.23\n",
            "[384 | 918.87] loss=0.47 avg=1.22\n",
            "[385 | 921.16] loss=0.45 avg=1.21\n",
            "[386 | 923.45] loss=0.54 avg=1.20\n",
            "[387 | 925.76] loss=0.66 avg=1.20\n",
            "[388 | 928.05] loss=0.67 avg=1.19\n",
            "[389 | 930.35] loss=0.51 avg=1.19\n",
            "[390 | 932.65] loss=0.43 avg=1.18\n",
            "[391 | 934.95] loss=0.62 avg=1.17\n",
            "[392 | 937.25] loss=0.52 avg=1.17\n",
            "[393 | 939.54] loss=0.56 avg=1.16\n",
            "[394 | 941.84] loss=0.29 avg=1.15\n",
            "[395 | 944.15] loss=0.44 avg=1.14\n",
            "[396 | 946.45] loss=0.42 avg=1.14\n",
            "[397 | 948.74] loss=0.71 avg=1.13\n",
            "[398 | 951.04] loss=0.38 avg=1.12\n",
            "[399 | 953.34] loss=0.35 avg=1.12\n",
            "[400 | 955.64] loss=0.43 avg=1.11\n",
            "======== SAMPLE 1 ========\n",
            " of the worst of all conditions; a whole herd of animals, in short, an ever-reappearing, ever-colliding, ever-reappearing, human being, from whom nothing else can come forth.\n",
            "\n",
            "187. There are two kinds of moral intuitions, those which we most frequently make in the name of morality, and those which we in general make when we boast of a \"higher nature\" or of \"higher intentions,\" as Darwin himself so eloquently and coldly and emotionlessly put it: those we most readily confine to ourselves, the kinds of morals we really are and the sorts of intuitions we confine to ourselves.\n",
            "\n",
            "188. With regard to the desire for \"free will\" and its connection with \"freedom of will,\" it is the worst sin to hold these notions for what you are generally proud of, even in your own bodies--the odium irium, for example, or the coccyx glauconidevium. On the other hand, with regard to the desire for \"intuition,\" it is generally a far more innocent sin, to assume, through ignorance and bravado, the whole conception of \"will\" in its purest form, in its most childlike form, and to overlook and over-interpret it, to overlook and over-interpret it, and above all to over-interpret it!\n",
            "\n",
            "189. The incurable will to \"make everything in my self a success\"--malice of the intellect against will, impulse against will--impels one to make terrible mistakes and even to invent a will of his own, in order to win sympathy from others: one is a dreadful artist who has always lost sight of the universal good (in fact, he is often too good to even be good).\n",
            "\n",
            "190. In the world of absolute truths, there are at least two types of monsters which all too readily can be branded: those who think in absolute sentences and are never to be retributors of the truth, the pedants, the \"the other\" (presumptively and necessarily)--are perhaps best known (along with the lizards) as the so-called hardy, timid, delicate, delicate, honest, and learned; the \"smart\"--who, along with their \"smartness,\" get the bad rep--also the reproachful \"courageous\"--also the self-injury--HAVING A TURNING ON, is a monster made worse and better by his error.\n",
            "\n",
            "191. One must consider against the grain the valuations which a person goes to the store every day. One must deal with his inclinations carefully and employ all the \"good conscience\" one has against him. If he is unable to divest himself of \"illness,\" he is at present most highly endowed and capable of sustaining and glorifying many small, harmless, yet very powerful, vivacious, vivacious loins; he is also most kindly, kind, and kindr to men--and in many cases, even of the most people, in the hands of the most modest, humble, and trivial beings. In addition to all this, his good graces and advances--be they from his very eyes, from a kindly, humble, and trivial reflective creature, or from a kindly, humble, and trivial being endowed throughout with the most instinctive and beards--are in reality only superficial gifts from the most \"good neighbour.\" From these \"good neighbours,\" as the learned person said, comes the moral term; and as little as the \"smallest\" neighbour may occupy the same \"house,\" according to the \"house-colour\" and \"house-tint\" of his \"fellow creatures,\" the same \"little house\" and \"little house\",--so is he (in the \"colour\" and \"colour\" of \"things\") a VEIL OF RANK.\n",
            "\n",
            "192. The more a race attains to genius, the more a right and a duty it has on its person; the more a man thinks and acts, the more a credit does he owe to the master, the more he is a hero;--the more is the hero \"forcibly\", by his own initiative, by the will, by the will of universal utility, by a will that for longs has stored and disguised the worst impulses, finally succumbed to them, and was in the end transcended only in appearance and power. The less a man is bound to his fellow man, the less he may know how to rely; he opens his heart to the overflowing goodness of the heart, he multiplies and develops it, till, turning only to the credit of his own Will, it asserts--and thus--its absolute sovereignty and supremacy.\n",
            "\n",
            "193. One must never resign one's judgment, just as I resign mine:\n",
            "194. \"Whom I distrust: it is impossible to be untruthful.\"--Ed.\n",
            "\n",
            "195. \"What I learned\n",
            "\n",
            "[401 | 968.42] loss=0.47 avg=1.10\n",
            "[402 | 970.71] loss=0.86 avg=1.10\n",
            "[403 | 973.01] loss=0.39 avg=1.09\n",
            "[404 | 975.30] loss=0.35 avg=1.09\n",
            "[405 | 977.60] loss=0.30 avg=1.08\n",
            "[406 | 979.91] loss=0.35 avg=1.07\n",
            "[407 | 982.20] loss=0.49 avg=1.06\n",
            "[408 | 984.49] loss=0.40 avg=1.06\n",
            "[409 | 986.79] loss=0.26 avg=1.05\n",
            "[410 | 989.08] loss=0.41 avg=1.04\n",
            "[411 | 991.38] loss=0.41 avg=1.04\n",
            "[412 | 993.68] loss=0.36 avg=1.03\n",
            "[413 | 995.98] loss=0.59 avg=1.03\n",
            "[414 | 998.27] loss=0.46 avg=1.02\n",
            "[415 | 1000.56] loss=0.34 avg=1.01\n",
            "[416 | 1002.85] loss=0.24 avg=1.01\n",
            "[417 | 1005.15] loss=0.49 avg=1.00\n",
            "[418 | 1007.44] loss=0.41 avg=0.99\n",
            "[419 | 1009.73] loss=0.37 avg=0.99\n",
            "[420 | 1012.03] loss=0.46 avg=0.98\n",
            "[421 | 1014.33] loss=0.26 avg=0.97\n",
            "[422 | 1016.63] loss=0.36 avg=0.97\n",
            "[423 | 1018.92] loss=0.38 avg=0.96\n",
            "[424 | 1021.22] loss=0.35 avg=0.96\n",
            "[425 | 1023.51] loss=0.36 avg=0.95\n",
            "[426 | 1025.80] loss=0.33 avg=0.94\n",
            "[427 | 1028.10] loss=0.23 avg=0.94\n",
            "[428 | 1030.41] loss=0.48 avg=0.93\n",
            "[429 | 1032.70] loss=0.45 avg=0.93\n",
            "[430 | 1035.00] loss=0.27 avg=0.92\n",
            "[431 | 1037.29] loss=0.55 avg=0.92\n",
            "[432 | 1039.59] loss=0.25 avg=0.91\n",
            "[433 | 1041.89] loss=0.46 avg=0.91\n",
            "[434 | 1044.18] loss=0.30 avg=0.90\n",
            "[435 | 1046.48] loss=0.26 avg=0.89\n",
            "[436 | 1048.78] loss=0.35 avg=0.89\n",
            "[437 | 1051.06] loss=0.35 avg=0.88\n",
            "[438 | 1053.37] loss=0.30 avg=0.88\n",
            "[439 | 1055.66] loss=0.34 avg=0.87\n",
            "[440 | 1057.96] loss=0.35 avg=0.87\n",
            "[441 | 1060.25] loss=0.31 avg=0.86\n",
            "[442 | 1062.54] loss=0.28 avg=0.85\n",
            "[443 | 1064.84] loss=0.36 avg=0.85\n",
            "[444 | 1067.14] loss=0.22 avg=0.84\n",
            "[445 | 1069.43] loss=0.23 avg=0.84\n",
            "[446 | 1071.73] loss=0.48 avg=0.83\n",
            "[447 | 1074.02] loss=0.30 avg=0.83\n",
            "[448 | 1076.32] loss=0.22 avg=0.82\n",
            "[449 | 1078.62] loss=0.41 avg=0.82\n",
            "[450 | 1080.92] loss=0.33 avg=0.81\n",
            "[451 | 1083.21] loss=0.25 avg=0.81\n",
            "[452 | 1085.51] loss=0.27 avg=0.80\n",
            "[453 | 1087.81] loss=0.35 avg=0.80\n",
            "[454 | 1090.11] loss=0.25 avg=0.79\n",
            "[455 | 1092.40] loss=0.24 avg=0.79\n",
            "[456 | 1094.71] loss=0.21 avg=0.78\n",
            "[457 | 1097.00] loss=0.22 avg=0.77\n",
            "[458 | 1099.30] loss=0.19 avg=0.77\n",
            "[459 | 1101.61] loss=0.31 avg=0.76\n",
            "[460 | 1103.91] loss=0.22 avg=0.76\n",
            "[461 | 1106.20] loss=0.24 avg=0.75\n",
            "[462 | 1108.49] loss=0.23 avg=0.75\n",
            "[463 | 1110.78] loss=0.18 avg=0.74\n",
            "[464 | 1113.08] loss=0.17 avg=0.74\n",
            "[465 | 1115.38] loss=0.32 avg=0.73\n",
            "[466 | 1117.67] loss=0.22 avg=0.73\n",
            "[467 | 1119.98] loss=0.22 avg=0.72\n",
            "[468 | 1122.27] loss=0.28 avg=0.72\n",
            "[469 | 1124.56] loss=0.28 avg=0.71\n",
            "[470 | 1126.87] loss=0.17 avg=0.71\n",
            "[471 | 1129.17] loss=0.21 avg=0.70\n",
            "[472 | 1131.47] loss=0.25 avg=0.70\n",
            "[473 | 1133.76] loss=0.20 avg=0.69\n",
            "[474 | 1136.06] loss=0.20 avg=0.69\n",
            "[475 | 1138.37] loss=0.22 avg=0.68\n",
            "[476 | 1140.67] loss=0.22 avg=0.68\n",
            "[477 | 1142.96] loss=0.30 avg=0.67\n",
            "[478 | 1145.26] loss=0.68 avg=0.67\n",
            "[479 | 1147.55] loss=0.25 avg=0.67\n",
            "[480 | 1149.85] loss=0.40 avg=0.67\n",
            "[481 | 1152.15] loss=0.42 avg=0.67\n",
            "[482 | 1154.45] loss=0.18 avg=0.66\n",
            "[483 | 1156.75] loss=0.19 avg=0.66\n",
            "[484 | 1159.05] loss=0.36 avg=0.65\n",
            "[485 | 1161.35] loss=0.27 avg=0.65\n",
            "[486 | 1163.65] loss=0.18 avg=0.64\n",
            "[487 | 1165.94] loss=0.18 avg=0.64\n",
            "[488 | 1168.24] loss=0.22 avg=0.64\n",
            "[489 | 1170.54] loss=0.26 avg=0.63\n",
            "[490 | 1172.84] loss=0.23 avg=0.63\n",
            "[491 | 1175.14] loss=0.24 avg=0.62\n",
            "[492 | 1177.44] loss=0.29 avg=0.62\n",
            "[493 | 1179.74] loss=0.22 avg=0.62\n",
            "[494 | 1182.04] loss=0.23 avg=0.61\n",
            "[495 | 1184.33] loss=0.32 avg=0.61\n",
            "[496 | 1186.64] loss=0.23 avg=0.61\n",
            "[497 | 1188.94] loss=0.17 avg=0.60\n",
            "[498 | 1191.24] loss=0.23 avg=0.60\n",
            "[499 | 1193.54] loss=0.27 avg=0.59\n",
            "[500 | 1195.84] loss=0.27 avg=0.59\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68733228-0ccf-4dce-ae5c-c9c2d87ebef2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "69. That which is national is best explained by the contrast it adopts\n",
            "between its neighbours and themselves. A people which\n",
            "its neighbour commands, its own greatness and its own\n",
            "necessity dictates how closely and profoundly the nation feels\n",
            "inherited values and how closely it is controlled and controlled\n",
            "by the state. A people that does not feel in its own interest any\n",
            "malice or crime that might result from its doing anything\n",
            "ascertained in the obedience of another, that feels itself bound and constrained\n",
            "by its neighbours, that is, it depends upon the former for its\n",
            "independence and its identity, or it may be led astray by the\n",
            "terrestrial, its limited and henceforward, its sensitive and\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}